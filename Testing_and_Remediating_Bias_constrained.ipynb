{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anilkumarpanda/fairness_cb/blob/development/Testing_and_Remediating_Bias_constrained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DHu_53EgMrTk",
      "metadata": {
        "id": "DHu_53EgMrTk"
      },
      "source": [
        "<a href=\"https://githubtocolab.com/ml-for-high-risk-apps-book/Machine-Learning-for-High-Risk-Applications-Book/blob/main/code/Chapter-10/Testing_and_Remediating_Bias_constrained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c98ab72-1169-40d7-bf5d-fd7c0eb12a14",
      "metadata": {
        "id": "2c98ab72-1169-40d7-bf5d-fd7c0eb12a14"
      },
      "source": [
        "# Fairness Code Breakfast : Testing and Remediating Bias in an XGBoost Credit Decision Model.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/anilkumarpanda/fairness_cb/development/img/adog.JPG\" alt=\"Alt Text\" height=\"400\">\n",
        "\n",
        "This AI generated dog image has nothing to do with Fairness, but I hope I have your attention now.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0KGo0YNaMNB0",
      "metadata": {
        "id": "0KGo0YNaMNB0"
      },
      "source": [
        "# Fairness in Machine Learning ?\n",
        "\n",
        "1. Is it important to consider fairness while developing models ?\n",
        "\n",
        "2. Can it be achieved via a model ?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f31d8e1-ca2f-4081-afc6-7332dcc43bc3",
      "metadata": {
        "id": "4f31d8e1-ca2f-4081-afc6-7332dcc43bc3"
      },
      "source": [
        "### 1. Setting the environment\n",
        "\n",
        "Download the data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/anilkumarpanda/fairness_cb/development/credit_line_increase.csv"
      ],
      "metadata": {
        "id": "njKqIwO8PNCg"
      },
      "id": "njKqIwO8PNCg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cl-o2WSxfjcR",
      "metadata": {
        "id": "cl-o2WSxfjcR",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Installing the libraries\n",
        "%pip install h2o\n",
        "%pip install shap\n",
        "%pip install 'XGBoost==1.6'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3415e2e2-657c-461f-96ba-8771f4173587",
      "metadata": {
        "id": "3415e2e2-657c-461f-96ba-8771f4173587",
        "tags": []
      },
      "source": [
        "## 2. Evaluating an XGBoost Model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cce747b-126f-4f98-b3c6-7f6096ac00c5",
      "metadata": {
        "id": "6cce747b-126f-4f98-b3c6-7f6096ac00c5"
      },
      "source": [
        "###  Excercise : Train a Credit Decision Model\n",
        "\n",
        "Run the code/cells till #3. Measure Fairness.\n",
        "We train a ML model, which some nice tricks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fcddefc-4827-4b7c-810c-85231a48bf06",
      "metadata": {
        "id": "5fcddefc-4827-4b7c-810c-85231a48bf06",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "import sklearn\n",
        "import shap\n",
        "np.random.seed(42)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a5d6485-4c74-43f1-aa64-c14efe0bcb22",
      "metadata": {
        "id": "0a5d6485-4c74-43f1-aa64-c14efe0bcb22",
        "tags": []
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/credit_line_increase.csv')\n",
        "data['SEX'] = np.where(data['SEX'] == 1, 'male', 'female')\n",
        "race_map = {1: 'hispanic', 2: 'black', 3: 'white', 4: 'asian'}\n",
        "data['RACE'] = data['RACE'].apply(lambda x: race_map[x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32daad4e-2e51-4924-aa02-70c3c56a32a0",
      "metadata": {
        "id": "32daad4e-2e51-4924-aa02-70c3c56a32a0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Modify the data so there is a distributional difference between borrowers of different race/ethnicities.\n",
        "\n",
        "new_limit_bal = data['LIMIT_BAL'] - 20000*np.random.randn(len(data))\n",
        "new_limit_bal[new_limit_bal <= 10000] = 10000\n",
        "data['LIMIT_BAL'] = np.where((data['RACE'] == 'hispanic') | (data['RACE'] == 'black'),\n",
        "                             new_limit_bal,\n",
        "                             data['LIMIT_BAL'])\n",
        "\n",
        "for i in range(1, 7):\n",
        "    delta = 1000*np.random.randn(len(data))\n",
        "    new_pay = data[f'PAY_AMT{i}'] - delta\n",
        "    new_pay[new_pay < 0] = 0\n",
        "\n",
        "    new_bill = data[f'BILL_AMT{i}'] - delta\n",
        "    new_bill[new_bill < 0] = 0\n",
        "\n",
        "    data[f'PAY_AMT{i}'] = np.where((data['RACE'] == 'hispanic') | (data['RACE'] == 'black'),\n",
        "                                   new_pay,\n",
        "                                   data[f'PAY_AMT{i}'])\n",
        "    data[f'BILL_AMT{i}'] = np.where((data['RACE'] == 'hispanic') | (data['RACE'] == 'black'),\n",
        "                                    new_bill,\n",
        "                                    data[f'BILL_AMT{i}'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7556f587-e2bf-4c22-b468-edbbb33d4f05",
      "metadata": {
        "id": "7556f587-e2bf-4c22-b468-edbbb33d4f05",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Split the data into train validation and test\n",
        "seed = 12345\n",
        "np.random.seed(seed)\n",
        "\n",
        "split_train_test = 2/3\n",
        "\n",
        "split = np.random.rand(len(data)) < split_train_test\n",
        "train = data[split].copy()\n",
        "test = data[~split].copy()\n",
        "\n",
        "split_test_valid = 1/2\n",
        "\n",
        "split = np.random.rand(len(test)) < split_test_valid\n",
        "valid = test[split].copy()\n",
        "test = test[~split].copy()\n",
        "\n",
        "del data\n",
        "\n",
        "print(f\"Train/Validation/Test sizes: {len(train)}/{len(valid)}/{len(test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7d5b7ad-871f-4d27-ab8e-da35e042250f",
      "metadata": {
        "id": "f7d5b7ad-871f-4d27-ab8e-da35e042250f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "id_col = 'ID'\n",
        "groups = ['SEX', 'RACE', 'EDUCATION', 'MARRIAGE', 'AGE']\n",
        "target = 'DELINQ_NEXT'\n",
        "features = [col for col in train.columns if col not in groups + [id_col, target]]\n",
        "\n",
        "dtrain = xgb.DMatrix(train[features],\n",
        "                     label=train[target])\n",
        "\n",
        "dvalid = xgb.DMatrix(valid[features],\n",
        "                     label=valid[target])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a585bb90-9e27-4e29-a3b0-2c7221abc771",
      "metadata": {
        "id": "a585bb90-9e27-4e29-a3b0-2c7221abc771",
        "tags": []
      },
      "outputs": [],
      "source": [
        "corr = pd.DataFrame(train[features + [target]].corr(method='spearman')[target]).iloc[:-1]\n",
        "\n",
        "def get_monotone_constraints(data, target, corr_threshold):\n",
        "    \"\"\"Calculate monotonic constraints.\n",
        "\n",
        "    Using a cutoff on Spearman correlation between features and target, return a tuple ready to pass into XGBoost.\n",
        "\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): A DataFrame containing the features in the order they appear to XGBoost, as well as the target variable.\n",
        "        target (str): The name of the column with the target variable in 'data'.\n",
        "        corr_threshold (float): The Spearman correlation threshold.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple with values in {-1, 0, 1}, where each element corresponds to a column in data (excluding the target itself). Ready to pass into xgb.train()\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    corr = pd.Series(data.corr(method='spearman')[target]).drop(target)\n",
        "    monotone_constraints = tuple(np.where(corr < -corr_threshold,\n",
        "                                          -1,\n",
        "                                          np.where(corr > corr_threshold,\n",
        "                                                   1,\n",
        "                                                   0)))\n",
        "    return monotone_constraints\n",
        "\n",
        "correlation_cutoff = 0.1\n",
        "\n",
        "monotone_constraints = get_monotone_constraints(train[features+[target]],\n",
        "                                                target,\n",
        "                                                correlation_cutoff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d02293f2-848c-4f1b-a3cd-0e5b79c62d70",
      "metadata": {
        "id": "d02293f2-848c-4f1b-a3cd-0e5b79c62d70",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Feed the model the global bias\n",
        "base_score = train[target].mean()\n",
        "\n",
        "params = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'auc',\n",
        "    'eta': 0.05,\n",
        "    'subsample': 0.6,\n",
        "    'colsample_bytree': 1.0,\n",
        "    'max_depth': 5,\n",
        "    'base_score': base_score,\n",
        "    'monotone_constraints': dict(zip(features, monotone_constraints)),\n",
        "    'seed': seed\n",
        "}\n",
        "\n",
        "# Train using early stopping on the validation dataset.\n",
        "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
        "\n",
        "model_constrained = xgb.train(params,\n",
        "                              dtrain,\n",
        "                              num_boost_round=200,\n",
        "                              evals=watchlist,\n",
        "                              early_stopping_rounds=10,\n",
        "                              verbose_eval=False)\n",
        "\n",
        "train[f'p_{target}'] = model_constrained.predict(dtrain)\n",
        "valid[f'p_{target}'] = model_constrained.predict(dvalid)\n",
        "test[f'p_{target}'] = model_constrained.predict(xgb.DMatrix(test[features], label=test[target]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dbcc992-de01-4568-9081-439097a39a73",
      "metadata": {
        "id": "4dbcc992-de01-4568-9081-439097a39a73",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Select the optimal probability cutoff by maximizing the F1 score on validation data.\n",
        "\n",
        "def perf_metrics(y_true, y_score, pos=1, neg=0, res=0.01):\n",
        "    \"\"\"\n",
        "    Calculates precision, recall, and f1 given outcomes and probabilities.\n",
        "\n",
        "    Args:\n",
        "        y_true: Array of binary outcomes\n",
        "        y_score: Array of assigned probabilities.\n",
        "        pos: Primary target value, default 1.\n",
        "        neg: Secondary target value, default 0.\n",
        "        res: Resolution by which to loop through cutoffs, default 0.01.\n",
        "\n",
        "    Returns:\n",
        "        Pandas dataframe of precision, recall, and f1 values.\n",
        "    \"\"\"\n",
        "\n",
        "    eps = 1e-20 # for safe numerical operations\n",
        "\n",
        "    # init p-r roc frame\n",
        "    prauc_frame = pd.DataFrame(columns=['cutoff', 'recall', 'precision', 'f1'])\n",
        "\n",
        "    # loop through cutoffs to create p-r roc frame\n",
        "    for cutoff in np.arange(0, 1 + res, res):\n",
        "\n",
        "        # binarize decision to create confusion matrix values\n",
        "        decisions = np.where(y_score > cutoff , 1, 0)\n",
        "\n",
        "        # calculate confusion matrix values\n",
        "        tp = np.sum((decisions == pos) & (y_true == pos))\n",
        "        fp = np.sum((decisions == pos) & (y_true == neg))\n",
        "        tn = np.sum((decisions == neg) & (y_true == neg))\n",
        "        fn = np.sum((decisions == neg) & (y_true == pos))\n",
        "\n",
        "        # calculate precision, recall, and f1\n",
        "        recall = (tp + eps)/((tp + fn) + eps)\n",
        "        precision = (tp + eps)/((tp + fp) + eps)\n",
        "        f1 = 2/((1/(recall + eps)) + (1/(precision + eps)))\n",
        "\n",
        "\n",
        "        # add new values to frame\n",
        "        prauc_frame = prauc_frame.append({'cutoff': cutoff,\n",
        "                                          'recall': recall,\n",
        "                                          'precision': precision,\n",
        "                                          'f1': f1},\n",
        "                                          ignore_index=True)\n",
        "\n",
        "    return prauc_frame\n",
        "\n",
        "\n",
        "model_metrics = perf_metrics(y_true=valid[target], y_score=model_constrained.predict(dvalid))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca0a2c92-f87d-4d6a-b9d9-fd6d64d53540",
      "metadata": {
        "id": "ca0a2c92-f87d-4d6a-b9d9-fd6d64d53540",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model_metrics.loc[model_metrics['f1'].idxmax()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1ddf410-ee18-477b-b363-6e9ed17303fc",
      "metadata": {
        "id": "e1ddf410-ee18-477b-b363-6e9ed17303fc",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Since the Disparate Impact Analysis(DIA) analysis will focus on model outcomes\n",
        "# (rather than scores), choose a cutoff in probability space.\n",
        "\n",
        "best_cut = model_metrics.loc[model_metrics['f1'].idxmax(), 'cutoff']\n",
        "best_cut_original = best_cut\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(model_metrics['cutoff'], model_metrics['precision'], label='Precision',linestyle='--')\n",
        "ax.plot(model_metrics['cutoff'], model_metrics['recall'], label='Recall',linestyle=':')\n",
        "ax.plot(model_metrics['cutoff'], model_metrics['f1'], label='F1 Score',linestyle='-.')\n",
        "ax.legend(loc=3)\n",
        "ax.set_xlabel('Score Cutoff')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "364ef8b5-e55b-4f0a-9258-8f053997b098",
      "metadata": {
        "id": "364ef8b5-e55b-4f0a-9258-8f053997b098",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def get_confusion_matrix(frame, y, yhat, by=None, level=None, cutoff=0.5):\n",
        "    \"\"\"\n",
        "    Creates confusion matrix from pandas dataframe of y and yhat values, can be sliced by a variable and level.\n",
        "\n",
        "    Args:\n",
        "        frame: Pandas dataframe of actual (y) and predicted (yhat) values.\n",
        "        y: Name of actual value column.\n",
        "        yhat: Name of predicted value column.\n",
        "        by: By variable to slice frame before creating confusion matrix, default None.\n",
        "        level: Value of by variable to slice frame before creating confusion matrix, default None.\n",
        "        cutoff: Cutoff threshold for confusion matrix, default 0.5.\n",
        "\n",
        "    Returns:\n",
        "        Confusion matrix as pandas dataframe.\n",
        "    \"\"\"\n",
        "\n",
        "    # determine levels of target (y) variable\n",
        "    # sort for consistency\n",
        "    level_list = list(frame[y].unique())\n",
        "    level_list.sort(reverse=True)\n",
        "\n",
        "    # init confusion matrix\n",
        "    cm_frame = pd.DataFrame(columns=['actual: ' +  str(i) for i in level_list],\n",
        "                            index=['predicted: ' + str(i) for i in level_list])\n",
        "\n",
        "    # don't destroy original data\n",
        "    frame_ = frame.copy(deep=True)\n",
        "\n",
        "    # convert numeric predictions to binary decisions using cutoff\n",
        "    dname = 'd_' + str(y)\n",
        "    frame_[dname] = np.where(frame_[yhat] > cutoff , 1, 0)\n",
        "\n",
        "    # slice frame\n",
        "    if (by is not None) & (level is not None):\n",
        "        frame_ = frame_[frame[by] == level]\n",
        "\n",
        "    # calculate size of each confusion matrix value\n",
        "    for i, lev_i in enumerate(level_list):\n",
        "        for j, lev_j in enumerate(level_list):\n",
        "            cm_frame.iat[j, i] = frame_[(frame_[y] == lev_i) & (frame_[dname] == lev_j)].shape[0]\n",
        "\n",
        "    return cm_frame\n",
        "\n",
        "get_confusion_matrix(test, target, f'p_{target}', cutoff=best_cut)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bHgxaXnOM78u",
      "metadata": {
        "id": "bHgxaXnOM78u"
      },
      "source": [
        "## 3. Measure Fairness\n",
        "\n",
        "\n",
        "The model you created above will be used provide loans. So it is important to create measure the models for fairness.\n",
        "\n",
        "In the domain of fairness, there a lot of metrics that can be considered depending on the problem.\n",
        "\n",
        "Some of them include :\n",
        "\n",
        "Prevalence: '(tp + fn) / (tp + tn +fp + fn)', # How much default actually happens for this group\n",
        "\n",
        "Accuracy: '(tp + tn) / (tp + tn + fp + fn)', # how often the model predicts default and non-default correctly for this group\n",
        "\n",
        "True Positive Rate: 'tp / (tp + fn)',  # out of the people in the group *that did* default, how many the model predicted *correctly* would default\n",
        "\n",
        "Precision: 'tp / (tp + fp)',  # out of the people in the group the model *predicted* would default, how many the model predicted *correctly* would default\n",
        "\n",
        "Specificity: 'tn / (tn + fp)', # out of the people in the group *that did not* default, how many the model predicted *correctly* would not default\n",
        "\n",
        "Negative Predicted Value: 'tn / (tn + fn)', # out of the people in the group the model *predicted* would not default, how many the model predicted *correctly* would not default\n",
        "\n",
        "False Positive Rate: 'fp / (tn + fp)', # out of the people in the group *that did not* default, how many the model predicted *incorrectly* would default\n",
        "\n",
        "False Discovery Rate: 'fp / (tp + fp)', # out of the people in the group the model *predicted* would default, how many the model predicted *incorrectly* would default\n",
        "\n",
        "False Negative Rate: 'fn / (tp + fn)', # out of the people in the group *that did* default, how many the model predicted *incorrectly* would not default\n",
        "\n",
        "False Omissions Rate: 'fn / (tn + fn)'  # out of the people in the group the model *predicted* would not default, how many the model predicted *incorrectly* would not default\n",
        "\n",
        "Are you feeling overwhelmed ??\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/anilkumarpanda/fairness_cb/development/img/blue_cat.JPG\" alt=\"Sad Bleu Cat\" height=\"400\">\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTkPZ1nf-X_zq_stiiNqv7opkVZU7wEJsMcjAhJSrq2ug&s\" alt=\"Andrew Ng\" height=\"400\">\n",
        "\n",
        "The Fairness Decision Tree can be a good starting point. ![fairness decision tree](https://raw.githubusercontent.com/anilkumarpanda/fairness_cb/development/fairness_tree.png)\n",
        "\n",
        "\n",
        "However for our case we will focus on AIR , SMD across groups.\n",
        "\n",
        "AIR : Adverse Impact Ratio\n",
        "\n",
        "SMD : Statistical Measures of Disparate Impact.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88c65147-1edb-476d-adc6-40bc01ff1be9",
      "metadata": {
        "id": "88c65147-1edb-476d-adc6-40bc01ff1be9"
      },
      "source": [
        "### Confusion Matrix Disparity Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ca6de54-cdab-48f0-9127-d25f786f2f07",
      "metadata": {
        "id": "5ca6de54-cdab-48f0-9127-d25f786f2f07",
        "tags": []
      },
      "outputs": [],
      "source": [
        "metric_dict = {\n",
        "'Prevalence': '(tp + fn) / (tp + tn +fp + fn)', # how much default actually happens for this group\n",
        "'Accuracy': '(tp + tn) / (tp + tn + fp + fn)', # how often the model predicts default and non-default correctly for this group\n",
        "'True Positive Rate': 'tp / (tp + fn)',  # out of the people in the group *that did* default, how many the model predicted *correctly* would default\n",
        "'Precision': 'tp / (tp + fp)',  # out of the people in the group the model *predicted* would default, how many the model predicted *correctly* would default\n",
        "'Specificity': 'tn / (tn + fp)', # out of the people in the group *that did not* default, how many the model predicted *correctly* would not default\n",
        "'Negative Predicted Value': 'tn / (tn + fn)', # out of the people in the group the model *predicted* would not default, how many the model predicted *correctly* would not default\n",
        "\n",
        "'False Positive Rate': 'fp / (tn + fp)', # out of the people in the group *that did not* default, how many the model predicted *incorrectly* would default\n",
        "'False Discovery Rate': 'fp / (tp + fp)', # out of the people in the group the model *predicted* would default, how many the model predicted *incorrectly* would default\n",
        "\n",
        "'False Negative Rate': 'fn / (tp + fn)', # out of the people in the group *that did* default, how many the model predicted *incorrectly* would not default\n",
        "'False Omissions Rate': 'fn / (tn + fn)'  # out of the people in the group the model *predicted* would not default, how many the model predicted *incorrectly* would not default\n",
        "}\n",
        "\n",
        "\n",
        "def confusion_matrix_parser(expression):\n",
        "\n",
        "    # tp | fp       cm_dict[level].iat[0, 0] | cm_dict[level].iat[0, 1]\n",
        "    # -------  ==>  --------------------------------------------\n",
        "    # fn | tn       cm_dict[level].iat[1, 0] | cm_dict[level].iat[1, 1]\n",
        "\n",
        "    expression = expression.replace('tp', 'cm_dict[level].iat[0, 0]')\\\n",
        "                           .replace('fp', 'cm_dict[level].iat[0, 1]')\\\n",
        "                           .replace('fn', 'cm_dict[level].iat[1, 0]')\\\n",
        "                           .replace('tn', 'cm_dict[level].iat[1, 1]')\n",
        "\n",
        "    return expression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b106abf-4ab1-4a4c-bedd-1421ecaaaee2",
      "metadata": {
        "id": "4b106abf-4ab1-4a4c-bedd-1421ecaaaee2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# initialize dict of confusion matrices and corresponding rows of dataframe\n",
        "sex_confusion_mats = {'male': get_confusion_matrix(test, target,\n",
        "                                                   f'p_{target}', by='SEX',\n",
        "                                                   level='male', cutoff=best_cut),\n",
        "                      'female': get_confusion_matrix(test, target,\n",
        "                                                     f'p_{target}', by='SEX',\n",
        "                                                     level='female', cutoff=best_cut)}\n",
        "\n",
        "def confusion_matrix_metrics(cm_dict, metric_dict):\n",
        "    levels = list(cm_dict.keys())\n",
        "\n",
        "    metrics_frame = pd.DataFrame(index=levels) # frame for metrics\n",
        "\n",
        "    for level in levels:\n",
        "        for metric in metric_dict.keys():\n",
        "\n",
        "            # parse metric expressions into executable pandas statements\n",
        "            expression = confusion_matrix_parser(metric_dict[metric])\n",
        "\n",
        "            # dynamically evaluate metrics to avoid code duplication\n",
        "            metrics_frame.loc[level, metric] = eval(expression)\n",
        "\n",
        "    return metrics_frame\n",
        "\n",
        "sex_confusion_metrics = confusion_matrix_metrics(sex_confusion_mats, metric_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60585b3e-67cb-452f-8716-85d2c9388d39",
      "metadata": {
        "id": "60585b3e-67cb-452f-8716-85d2c9388d39",
        "tags": []
      },
      "outputs": [],
      "source": [
        "sex_confusion_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qZq1a43DVVTw",
      "metadata": {
        "id": "qZq1a43DVVTw"
      },
      "source": [
        "### Excercise :\n",
        "1. Calculate metrics across Race groups.\n",
        "2. When do we know if the metrics are in red ?\n",
        "3. Can you highlight what values are in red ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6f8825f-528b-4ad6-a05e-6f02c9554123",
      "metadata": {
        "id": "b6f8825f-528b-4ad6-a05e-6f02c9554123",
        "tags": [],
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Excerise : Confusion Matrix for Race Group\n",
        "\n",
        "race_levels = list(race_map.values())\n",
        "race_confusion_mats = {level: get_confusion_matrix(test, target, f'p_{target}', by='RACE',\n",
        "                                                   level=level, cutoff=best_cut) for level in race_levels}\n",
        "race_confusion_metrics = confusion_matrix_metrics(race_confusion_mats, metric_dict)\n",
        "race_confusion_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0aac8417-d6c0-479a-b763-13787da2d086",
      "metadata": {
        "id": "0aac8417-d6c0-479a-b763-13787da2d086",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title Confusion Matrix for Race Group : Highlight disparity\n",
        "\n",
        "race_disparity_frame = race_confusion_metrics/race_confusion_metrics.loc['white', :]\n",
        "race_disparity_frame.columns=[col + ' Disparity' for col in race_confusion_metrics.columns]\n",
        "\n",
        "# small utility function to format pandas table output\n",
        "def disparate_red(val, parity_threshold_low=0.8, parity_threshold_hi=1.25):\n",
        "    color = 'grey' if (parity_threshold_low < val < parity_threshold_hi) else 'red'\n",
        "    return 'color: %s' % color\n",
        "\n",
        "race_disparity_frame.style.applymap(disparate_red)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab269477-90f9-4eb6-a072-6b2a5dfc46c2",
      "metadata": {
        "id": "ab269477-90f9-4eb6-a072-6b2a5dfc46c2",
        "tags": [],
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Confusion Matrix for Gender Group : Highlight disparity\n",
        "\n",
        "sex_disparity_frame = sex_confusion_metrics/sex_confusion_metrics.loc['male', :]\n",
        "sex_disparity_frame.columns=[col + ' Disparity' for col in sex_confusion_metrics.columns]\n",
        "\n",
        "# small utility function to format pandas table output\n",
        "def disparate_red(val, parity_threshold_low=0.8, parity_threshold_hi=1.25):\n",
        "    color = 'grey' if (parity_threshold_low < val < parity_threshold_hi) else 'red'\n",
        "    return 'color: %s' % color\n",
        "\n",
        "sex_disparity_frame.style.applymap(disparate_red)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee794dd6-c94d-4b4c-a888-6ba453b8952b",
      "metadata": {
        "id": "ee794dd6-c94d-4b4c-a888-6ba453b8952b"
      },
      "source": [
        "### Fair Lending Disparity Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80fe3d51-fce9-4f17-9c6b-b17cccb887d1",
      "metadata": {
        "id": "80fe3d51-fce9-4f17-9c6b-b17cccb887d1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from scipy.stats import ttest_ind, chisquare, fisher_exact, chi2_contingency\n",
        "\n",
        "\n",
        "# def air_statistical_signif(group_count, group_favorable, reference_count, reference_favorable):\n",
        "#     # Perform a chi-square test\n",
        "#     # (or Fisher's exact when cells in the contingency test have less than 30 individuals in them).\n",
        "\n",
        "#     group_unfavorable = group_count - group_favorable\n",
        "#     reference_unfavorable = reference_count - reference_favorable\n",
        "\n",
        "#     contingency_table = np.array([[group_favorable, group_unfavorable],\n",
        "#                                   [reference_favorable, reference_unfavorable]])\n",
        "\n",
        "#     if np.min(contingency_table) < 30:\n",
        "#         _, p = fisher_exact(contingency_table)\n",
        "#     else:\n",
        "#         _, p, _, _ = chi2_contingency(contingency_table)\n",
        "\n",
        "#     return p\n",
        "\n",
        "# def smd_statistical_signif(group_scores, reference_scores):\n",
        "#     # Perform a one-sided t-test. An outcome of 1 is assumed to be favorable.\n",
        "\n",
        "#     # We do not assume that the two scores have equal variance. Furthermore, we are testing\n",
        "#     # against the alternative hypothesis that the group receives lower scores than the reference\n",
        "#     # group.\n",
        "#     _, p = ttest_ind(group_scores, reference_scores, equal_var=False, alternative='less')\n",
        "#     return p\n",
        "\n",
        "\n",
        "def fair_lending_disparity(frame, y, yhat, demo_name, groups, reference_group, cutoff=0.5, favorable_outcome=0):\n",
        "    \"\"\"\n",
        "    Creates a table of fair lending disparity metrics (AIR and SMD).\n",
        "\n",
        "    Args:\n",
        "        frame: Pandas dataframe of actual (y) and predicted (yhat) values with group membership.\n",
        "        y: Name of actual value column.\n",
        "        yhat: Name of predicted value column.\n",
        "        demo_name: The name of the column containing the group information\n",
        "        groups: The names of the groups in the demo_name column.\n",
        "        reference_group: The control group.\n",
        "        cutoff: Cutoff threshold for confusion matrix, default 0.5.\n",
        "        favorable_outcome: The value {0, 1} that corresponds to a desirable outcome.\n",
        "\n",
        "    Returns:\n",
        "        A DataFrame summarizing the fair lending metrics analysis\n",
        "    \"\"\"\n",
        "\n",
        "    protected_groups = [group for group in groups if group != reference_group]\n",
        "    groups_ordered = protected_groups + [reference_group]\n",
        "\n",
        "    temp_frame = frame.copy()\n",
        "    temp_frame['model_outcome'] = np.where(temp_frame[yhat] <= cutoff, 0, 1)\n",
        "    temp_frame['fav_outcome'] = temp_frame['model_outcome'] == favorable_outcome\n",
        "    temp_frame['fav_score'] = temp_frame[yhat] if favorable_outcome else 1-temp_frame[yhat]\n",
        "\n",
        "    disparity_table = pd.DataFrame(index=groups_ordered)\n",
        "\n",
        "    disparity_table['Count'] = [len(temp_frame.loc[temp_frame[demo_name] == group]) for group in groups_ordered]\n",
        "    disparity_table['Favorable Outcomes'] = [temp_frame.loc[temp_frame[demo_name] == group]['fav_outcome'].sum()\n",
        "                                             for group in groups_ordered]\n",
        "    disparity_table['Favorable Rate'] = [disparity_table['Favorable Outcomes'][group]/disparity_table['Count'][group]\n",
        "                                         for group in groups_ordered]\n",
        "    disparity_table['Mean Score'] = [temp_frame.loc[temp_frame[demo_name] == group][yhat].mean()\n",
        "                                             for group in groups_ordered]\n",
        "    disparity_table['Std Score'] = [temp_frame.loc[temp_frame[demo_name].isin([reference_group, group])][yhat].std()\n",
        "                                             for group in groups_ordered]\n",
        "    try:\n",
        "        disparity_table['AIR'] = [disparity_table['Favorable Rate'][group]/disparity_table['Favorable Rate'][reference_group]\n",
        "                                  for group in groups_ordered]\n",
        "    except:\n",
        "        disparity_table['AIR'] = np.nan\n",
        "\n",
        "    # disparity_table['AIR p-value'] = [air_statistical_signif(disparity_table['Count'][group],\n",
        "    #                                                          disparity_table['Favorable Outcomes'][group],\n",
        "    #                                                          disparity_table['Count'][reference_group],\n",
        "    #                                                          disparity_table['Favorable Outcomes'][reference_group])\n",
        "    #                                   for group in groups_ordered]\n",
        "\n",
        "    disparity_table['SMD'] = [(disparity_table['Mean Score'][group] -\n",
        "                               disparity_table['Mean Score'][reference_group]) /\n",
        "                              disparity_table['Std Score'][group]\n",
        "                              for group in groups_ordered]\n",
        "\n",
        "    # disparity_table['SMD p-value'] = [smd_statistical_signif(temp_frame.loc[temp_frame[demo_name] == group]['fav_score'],\n",
        "    #                                                          temp_frame.loc[temp_frame[demo_name] == reference_group]['fav_score'])\n",
        "    #                                   for group in groups_ordered]\n",
        "\n",
        "    return disparity_table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92f6081a-51be-4c72-b1f1-3f57b63f48a6",
      "metadata": {
        "id": "92f6081a-51be-4c72-b1f1-3f57b63f48a6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Calculate the AIR and SMD scores across Train, Test & Validation datasets.\n",
        "\n",
        "fair_lending_disparity(train, y=target, yhat=f'p_{target}',\n",
        "                       demo_name='RACE', groups=race_levels, reference_group='white',\n",
        "                       cutoff=best_cut)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90488eb6-92b6-4e6c-8de9-de78219bba1f",
      "metadata": {
        "id": "90488eb6-92b6-4e6c-8de9-de78219bba1f",
        "tags": [],
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Excercise : Disparity for validation set\n",
        "fair_lending_disparity(valid, y=target, yhat=f'p_{target}',\n",
        "                       demo_name='RACE', groups=race_levels, reference_group='white',\n",
        "                       cutoff=best_cut)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50553a58-eab2-4ea8-9a59-5d3ca5cdc28f",
      "metadata": {
        "id": "50553a58-eab2-4ea8-9a59-5d3ca5cdc28f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title Excersise : Disparity for test set\n",
        "\n",
        "fair_lending_disparity(test, y=target, yhat=f'p_{target}',\n",
        "                       demo_name='RACE', groups=race_levels, reference_group='white',\n",
        "                       cutoff=best_cut)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d573b664-4ae3-4425-903e-08a28b19b11d",
      "metadata": {
        "id": "d573b664-4ae3-4425-903e-08a28b19b11d"
      },
      "source": [
        "### Residual Fairness Analysis\n",
        "\n",
        "We can also simply check the residual of the model, to see how does error relate to within each group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71ae4f2b-32c6-4722-945a-b495f806e3b4",
      "metadata": {
        "id": "71ae4f2b-32c6-4722-945a-b495f806e3b4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def plot_group_residuals(frame, y, yhat, demo_name, groups, cutoff=0.5, favorable_outcome=0):\n",
        "    \"\"\"\n",
        "    Generates a plot of logloss residuals for individuals who falsely received an unfavorable outcome.\n",
        "\n",
        "    Args:\n",
        "        frame: Pandas dataframe of actual (y) and predicted (yhat) values with group membership.\n",
        "        y: Name of actual value column.\n",
        "        yhat: Name of predicted value column.\n",
        "        demo_name: The name of the column containing the group information\n",
        "        groups: The names of the groups in the demo_name column.\n",
        "        cutoff: Cutoff threshold for confusion matrix, default 0.5.\n",
        "        favorable_outcome: The value {0, 1} that corresponds to a desirable outcome.\n",
        "\n",
        "    Returns:\n",
        "        Axes of the plot\n",
        "    \"\"\"\n",
        "\n",
        "    fig, axs = plt.subplots(1, len(groups), figsize=(5*len(groups), 5))\n",
        "    for i, group in enumerate(groups):\n",
        "        group_frame = frame.loc[frame[demo_name] == group].copy()\n",
        "        group_frame[f'{yhat}_outcome'] = np.where(group_frame[yhat] > cutoff, 1, 0)\n",
        "\n",
        "        group_frame[f'{y}_residual'] = -group_frame[y]*np.log(group_frame[yhat])\n",
        "        group_frame[f'{y}_residual'] -= (1 - group_frame[y])*np.log(1 - group_frame[yhat])\n",
        "\n",
        "        misclassified_obs = group_frame[(group_frame[y] == favorable_outcome) &\n",
        "                                        (group_frame[f'{yhat}_outcome'] == (1 - favorable_outcome))]\n",
        "        axs[i].scatter(misclassified_obs[yhat], misclassified_obs[f'{y}_residual'], marker='o', alpha=0.3)\n",
        "        axs[i].set_xlabel('Score')\n",
        "        axs[i].set_ylabel('Residual')\n",
        "        axs[i].set_title(group)\n",
        "        axs[i].set_xlim([0, 1])\n",
        "        axs[i].set_ylim([0, 2])\n",
        "\n",
        "    fig.suptitle('Logloss Residuals')\n",
        "\n",
        "    return axs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7184ab45-6245-4e3a-9e8c-8b48d53bd549",
      "metadata": {
        "id": "7184ab45-6245-4e3a-9e8c-8b48d53bd549",
        "tags": []
      },
      "outputs": [],
      "source": [
        "_ = plot_group_residuals(test, y=target, yhat=f'p_{target}',\n",
        "                       demo_name='RACE', groups=race_levels,\n",
        "                       cutoff=best_cut)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Excersie : Residual Plot for Gender\n",
        "\n",
        "gender_levels = [] # define gender level as list ['male','female']\n",
        "\n",
        "if len(gender_levels) ==0 :\n",
        "  print(\"Define gender levels\")\n",
        "else :\n",
        "  _ = plot_group_residuals(test, y=target, yhat=f'p_{target}',\n",
        "                       demo_name='SEX', groups=gender_levels,\n",
        "                       cutoff=best_cut)"
      ],
      "metadata": {
        "id": "G-FkaUCcgkuf"
      },
      "id": "G-FkaUCcgkuf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9735f488-77f4-41f8-9bd1-56dd31760351",
      "metadata": {
        "id": "9735f488-77f4-41f8-9bd1-56dd31760351"
      },
      "source": [
        "## 3. Remediating Model Bias\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/anilkumarpanda/fairness_cb/development/img/ds_cat.JPG\" alt=\"Alt Text\" height=\"400\">\n",
        "\n",
        "A cat data scientist at work.\n",
        "\n",
        "There are many potential ways to remediate bias. Some of the most common include pre-, in-, and postprocessing, and model selection:\n",
        "\n",
        "1. Preprocessing : Rebalancing, reweighing, or resampling training data so that demographic groups are better represented or positive outcomes are distributed more equitably.\n",
        "\n",
        "2. In-processing : Any number of alterations to ML training algorithms, including constraints, regularization and dual loss functions, or incorporation of adversarial modeling information, that attempt to generate more balanced outputs or performance across demographic groups.\n",
        "\n",
        "3. Postprocessing :Changing model predictions directly to create less biased outcomes.\n",
        "\n",
        "4. Model selection : Considering bias along with performance when selecting models. Typically, itâ€™s possible to find a model with good performance and fairness characteristics if we measure bias and performance across a large set of hyperparameter settings and input features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d34102ca-88d9-4234-8801-f74ab803a83b",
      "metadata": {
        "id": "d34102ca-88d9-4234-8801-f74ab803a83b"
      },
      "source": [
        "### Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e3681b8-3826-48e5-a41b-e0c8de234558",
      "metadata": {
        "id": "8e3681b8-3826-48e5-a41b-e0c8de234558",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def reweight_dataset(frame, y, demo_name, groups):\n",
        "    \"\"\"\n",
        "    Generates a weight for each observation according to the reweighting algorithm of\n",
        "    Kamiran and Kalders 2012, Data preprocessing techniques for classification without discrimination.\n",
        "\n",
        "    Args:\n",
        "        frame: Pandas dataframe of actual (y) and group information.\n",
        "        y: Name of actual value column (assumed to be binary).\n",
        "        demo_name: The name of the column containing the group information\n",
        "        groups: The names of the groups in the demo_name column.\n",
        "\n",
        "    Returns:\n",
        "        A Series containing the new observation weights.\n",
        "    \"\"\"\n",
        "\n",
        "    n = len(frame)\n",
        "\n",
        "    freq_dict = {'pos': len(frame.loc[frame[y] == 1])/n,\n",
        "                 'neg': len(frame.loc[frame[y] == 0])/n}\n",
        "\n",
        "    freq_dict.update({group: frame[demo_name].value_counts()[group]/n for group in groups})\n",
        "\n",
        "    weights = pd.Series(np.ones(n), index=frame.index)\n",
        "\n",
        "    for label in [0, 1]:\n",
        "        for group in groups:\n",
        "            label_name = 'pos' if label == 1 else 'neg'\n",
        "            freq = frame.loc[frame[y] == label][demo_name].value_counts()[group]/n\n",
        "            weights[(frame[y] == label) & (frame[demo_name] == group)] *= freq_dict[group]*freq_dict[label_name]/freq\n",
        "\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f86a19c5-7e7f-4af2-8a56-790e4e59ff91",
      "metadata": {
        "id": "f86a19c5-7e7f-4af2-8a56-790e4e59ff91",
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_weights = reweight_dataset(train, target, 'RACE', race_levels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61aa0e96-bf79-4e63-a259-d139a92110c7",
      "metadata": {
        "id": "61aa0e96-bf79-4e63-a259-d139a92110c7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "for race in race_levels:\n",
        "    print(f\"Mean outcome for {race}: {np.round(train.loc[train['RACE'] == race][target].mean(), 3)}\")\n",
        "\n",
        "    weighted_target = np.multiply(train.loc[train['RACE'] == race][target],\n",
        "                                  train_weights.loc[train['RACE'] == race])\n",
        "\n",
        "    print(f\"Mean outcome for {race} - reweighted: {np.round(weighted_target.mean(), 3)} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62333804-55b3-46ad-a941-20186e8376be",
      "metadata": {
        "id": "62333804-55b3-46ad-a941-20186e8376be",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dtrain = xgb.DMatrix(train[features],\n",
        "                     label=train[target],\n",
        "                     weight=train_weights)\n",
        "\n",
        "\n",
        "model_reweighted = xgb.train(params,\n",
        "                             dtrain,\n",
        "                             num_boost_round=100,\n",
        "                             evals=watchlist,\n",
        "                             early_stopping_rounds=10,\n",
        "                             verbose_eval=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96280603-a9e9-4106-87cb-0c85a9bee0c9",
      "metadata": {
        "id": "96280603-a9e9-4106-87cb-0c85a9bee0c9",
        "tags": []
      },
      "outputs": [],
      "source": [
        "reweighted_model_metrics = perf_metrics(y_true=valid[target], y_score=model_reweighted.predict(dvalid))\n",
        "reweighted_best_cut = reweighted_model_metrics.loc[reweighted_model_metrics['f1'].idxmax(), 'cutoff']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b07bea-a72d-486e-8196-51c8163ff324",
      "metadata": {
        "id": "a6b07bea-a72d-486e-8196-51c8163ff324",
        "tags": []
      },
      "outputs": [],
      "source": [
        "test[f'p_{target}_reweighted'] = model_reweighted.predict(xgb.DMatrix(test[features], label=test[target]))\n",
        "\n",
        "fair_lending_disparity(test, y=target, yhat=f'p_{target}_reweighted',\n",
        "                       demo_name='RACE', groups=race_levels, reference_group='white',\n",
        "                       cutoff=reweighted_best_cut)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b057da1-ef9d-4d09-909a-f39f6f7607f9",
      "metadata": {
        "id": "1b057da1-ef9d-4d09-909a-f39f6f7607f9",
        "tags": []
      },
      "outputs": [],
      "source": [
        "train[f'p_{target}_reweighted'] = model_reweighted.predict(dtrain)\n",
        "\n",
        "fair_lending_disparity(train, y=target, yhat=f'p_{target}_reweighted',\n",
        "                       demo_name='RACE', groups=race_levels, reference_group='white',\n",
        "                       cutoff=reweighted_best_cut)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "626ced50-3702-4694-a3d9-f68c41a4ec7c",
      "metadata": {
        "id": "626ced50-3702-4694-a3d9-f68c41a4ec7c",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def make_fair_objective(protected, lam):\n",
        "    def fair_objective(pred, dtrain):\n",
        "        \"\"\"\n",
        "        Fairness-aware cross-entropy loss objective function\n",
        "        \"\"\"\n",
        "        label = dtrain.get_label()\n",
        "        pred = 1.0 / (1.0 + np.exp(-pred))\n",
        "        grad = (pred - label) - lam * (pred - protected)\n",
        "        hess = (1 - lam) * pred * (1. - pred)\n",
        "\n",
        "        return grad, hess\n",
        "    return fair_objective\n",
        "\n",
        "protected = np.where((train['RACE'] == 'hispanic') | (train['RACE'] == 'black'), 1, 0)\n",
        "fair_objective = make_fair_objective(protected, lam=0.2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "113e19ce-065b-45b1-8bd0-5d0a2169640a",
      "metadata": {
        "id": "113e19ce-065b-45b1-8bd0-5d0a2169640a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dtrain = xgb.DMatrix(train[features],\n",
        "                     label=train[target])\n",
        "\n",
        "\n",
        "# Train using early stopping on the validation dataset.\n",
        "watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
        "\n",
        "model_regularized = xgb.train(params,\n",
        "                              dtrain,\n",
        "                              num_boost_round=100,\n",
        "                              evals=watchlist,\n",
        "                              early_stopping_rounds=10,\n",
        "                              verbose_eval=False,\n",
        "                              obj=fair_objective)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4779855-8195-40c9-bb4e-0174ac68c154",
      "metadata": {
        "id": "d4779855-8195-40c9-bb4e-0174ac68c154",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model_metrics = perf_metrics(y_true=valid[target], y_score=model_regularized.predict(dvalid))\n",
        "best_cut_regularized = model_metrics.loc[model_metrics['f1'].idxmax(), 'cutoff']\n",
        "\n",
        "test[f'p_{target}_regularized'] = model_regularized.predict(xgb.DMatrix(test[features], label=test[target]))\n",
        "\n",
        "fair_lending_disparity(test, y=target, yhat=f'p_{target}_regularized',\n",
        "                       demo_name='RACE', groups=race_levels, reference_group='white',\n",
        "                       cutoff=best_cut_regularized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1924aa2-4c9d-40c5-92b8-91de47ca61da",
      "metadata": {
        "id": "a1924aa2-4c9d-40c5-92b8-91de47ca61da",
        "tags": []
      },
      "outputs": [],
      "source": [
        "train[f'p_{target}_regularized'] = model_regularized.predict(xgb.DMatrix(train[features], label=train[target]))\n",
        "\n",
        "fair_lending_disparity(train, y=target, yhat=f'p_{target}_regularized',\n",
        "                       demo_name='RACE', groups=race_levels, reference_group='white',\n",
        "                       cutoff=best_cut_regularized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84ae72f7-9aea-47e7-97be-c701531a6138",
      "metadata": {
        "id": "84ae72f7-9aea-47e7-97be-c701531a6138",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Generate plots of model accuracy (F1 score), number of trees,\n",
        "# black AIR, hispanic AIR, and asian AIR as a\n",
        "# function of the regularization hyperparameter.\n",
        "\n",
        "lams = np.arange(0, 1, 0.10)\n",
        "scores = []\n",
        "trees = []\n",
        "\n",
        "air_black = []\n",
        "air_hispanic = []\n",
        "air_asian = []\n",
        "\n",
        "dtest = xgb.DMatrix(test[features], label=test[target])\n",
        "\n",
        "for lam in lams:\n",
        "\n",
        "    model = xgb.train(params,\n",
        "                      dtrain,\n",
        "                      num_boost_round=200,\n",
        "                      evals=watchlist,\n",
        "                      early_stopping_rounds=10,\n",
        "                      verbose_eval=False,\n",
        "                      obj=make_fair_objective(protected, lam=lam))\n",
        "\n",
        "    model_metrics = perf_metrics(y_true=valid[target],\n",
        "                                 y_score=model.predict(dvalid, iteration_range=(0, model.best_iteration)))\n",
        "    best_cut = model_metrics.loc[model_metrics['f1'].idxmax(), 'cutoff']\n",
        "\n",
        "    test[f'p_{target}_lam_test'] = model.predict(dtest, iteration_range=(0, model.best_iteration))\n",
        "\n",
        "    air_table = fair_lending_disparity(test, y=target, yhat=f'p_{target}_lam_test',\n",
        "                                       demo_name='RACE', groups=race_levels, reference_group='white',\n",
        "                                       cutoff=best_cut)\n",
        "\n",
        "    scores.append(model_metrics['f1'].max())\n",
        "    trees.append(model.best_ntree_limit)\n",
        "\n",
        "    air_black.append(air_table.loc['black']['AIR'])\n",
        "    air_hispanic.append(air_table.loc['hispanic']['AIR'])\n",
        "    air_asian.append(air_table.loc['asian']['AIR'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98499a76-1e90-44c4-ab53-1711fc85e023",
      "metadata": {
        "id": "98499a76-1e90-44c4-ab53-1711fc85e023",
        "tags": []
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "axs[0].plot(lams, [score/scores[0] for score in scores])\n",
        "axs[0].set_xlabel('Lambda')\n",
        "axs[0].set_ylabel(f'F1 Score \\n Relative to original')\n",
        "\n",
        "axs[1].plot(lams, trees)\n",
        "axs[1].set_xlabel('Lambda')\n",
        "axs[1].set_ylabel('Num Trees')\n",
        "\n",
        "#fig.savefig('../Data/Data/Figures/inprocessing_summary_i.svg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7919b2eb-731c-4ede-ba4e-bfbcbbda58bc",
      "metadata": {
        "id": "7919b2eb-731c-4ede-ba4e-bfbcbbda58bc",
        "tags": []
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
        "axs[0].plot(lams, air_black)\n",
        "axs[0].set_xlabel('Lambda')\n",
        "axs[0].set_ylabel('AIR (black)')\n",
        "axs[0].set_ylim((air_black[0]-0.05, 1.05))\n",
        "\n",
        "axs[1].plot(lams, air_hispanic)\n",
        "axs[1].set_xlabel('Lambda')\n",
        "axs[1].set_ylabel('AIR (Hispanic)')\n",
        "axs[1].set_ylim((air_hispanic[0]-0.05, 1.05))\n",
        "\n",
        "\n",
        "axs[2].plot(lams, air_asian)\n",
        "axs[2].set_xlabel('Lambda')\n",
        "axs[2].set_ylabel('AIR (asian)')\n",
        "axs[2].set_ylim((air_hispanic[0]-0.05, 1.05))\n",
        "\n",
        "#fig.savefig('../Data/Data/Figures/inprocessing_summary_ii.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YDhTF_3rjMyu",
      "metadata": {
        "id": "YDhTF_3rjMyu"
      },
      "source": [
        "### Excercise\n",
        "1. How does re-weighting the classes affect AIR ?\n",
        "2. Can you think of possible scenarios where re-weighting the data would help ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec6ada4-eee4-4cb4-8149-1770b4a33c52",
      "metadata": {
        "id": "fec6ada4-eee4-4cb4-8149-1770b4a33c52",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title Excercise : Reweighting Scheme and its effect on AIR\n",
        "\n",
        "# Conduct a similar experiment with the magnitude of the reweighing scheme.\n",
        "\n",
        "magnitudes = np.arange(0, 1.5, 0.10)\n",
        "scores = []\n",
        "trees = []\n",
        "\n",
        "air_black_ii = []\n",
        "air_hispanic_ii = []\n",
        "air_asian_ii = []\n",
        "\n",
        "dtest = xgb.DMatrix(test[features], label=test[target])\n",
        "\n",
        "for mag in magnitudes:\n",
        "\n",
        "    weight_change = 1 - train_weights\n",
        "    new_train_weights = 1 - mag*weight_change\n",
        "\n",
        "    dtrain = xgb.DMatrix(train[features],\n",
        "                     label=train[target],\n",
        "                     weight=new_train_weights)\n",
        "\n",
        "\n",
        "    model = xgb.train(params,\n",
        "                      dtrain,\n",
        "                      num_boost_round=200,\n",
        "                      evals=watchlist,\n",
        "                      early_stopping_rounds=10,\n",
        "                      verbose_eval=False)\n",
        "\n",
        "    model_metrics = perf_metrics(y_true=valid[target],\n",
        "                                 y_score=model.predict(dvalid, iteration_range=(0, model.best_iteration)))\n",
        "    best_cut = model_metrics.loc[model_metrics['f1'].idxmax(), 'cutoff']\n",
        "\n",
        "    test[f'p_{target}_lam_test'] = model.predict(dtest, iteration_range=(0, model.best_iteration))\n",
        "\n",
        "    air_table = fair_lending_disparity(test, y=target, yhat=f'p_{target}_lam_test',\n",
        "                                       demo_name='RACE', groups=race_levels, reference_group='white',\n",
        "                                       cutoff=best_cut)\n",
        "\n",
        "    scores.append(model_metrics['f1'].max())\n",
        "    trees.append(model.best_ntree_limit)\n",
        "\n",
        "    air_black_ii.append(air_table.loc['black']['AIR'])\n",
        "    air_hispanic_ii.append(air_table.loc['hispanic']['AIR'])\n",
        "    air_asian_ii.append(air_table.loc['asian']['AIR'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31374fa6-5eb8-4a55-85fd-3504e05acb93",
      "metadata": {
        "id": "31374fa6-5eb8-4a55-85fd-3504e05acb93",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title Excersise : Reweighting effect on model performance\n",
        "\n",
        "# This plot shows a relative drop in F1 score down to 94% of the original value.\n",
        "# Meanwhile, black and Hispanic AIRs go from 0.72 to 0.9!\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "axs[0].plot(magnitudes, [score/scores[0] for score in scores])\n",
        "axs[0].set_xlabel('Reweighing Strength')\n",
        "axs[0].set_ylabel('F1 Score')\n",
        "\n",
        "axs[1].plot(magnitudes, trees)\n",
        "axs[1].set_xlabel('Reweighing Strength')\n",
        "axs[1].set_ylabel('Num Trees')\n",
        "\n",
        "#fig.savefig('../Data/Data/Figures/preprocessing_summary_i.svg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8359672-2378-4a9d-a46d-2622a0007718",
      "metadata": {
        "id": "b8359672-2378-4a9d-a46d-2622a0007718",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# @title Excersise : Reweighting effect on AIR across Race groups\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "axs[0].plot(magnitudes, air_black_ii)\n",
        "axs[0].set_xlabel('Reweighing Strength')\n",
        "axs[0].set_ylabel('AIR (black)')\n",
        "axs[0].set_ylim((air_black_ii[0]-0.05, 1.05))\n",
        "\n",
        "axs[1].plot(magnitudes, air_hispanic_ii)\n",
        "axs[1].set_xlabel('Reweighing Strength')\n",
        "axs[1].set_ylabel('AIR (Hispanic)')\n",
        "axs[1].set_ylim((air_hispanic_ii[0]-0.05, 1.05))\n",
        "\n",
        "axs[2].plot(magnitudes, air_asian_ii)\n",
        "axs[2].set_xlabel('Reweighing Strength')\n",
        "axs[2].set_ylabel('AIR (asian)')\n",
        "axs[2].set_ylim((air_hispanic[0]-0.05, 1.05))\n",
        "\n",
        "#fig.savefig('../Data/Data/Figures/preprocessing_summary_ii.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3946b4b0-f228-4359-a0fd-6e4375048148",
      "metadata": {
        "id": "3946b4b0-f228-4359-a0fd-6e4375048148"
      },
      "source": [
        "## 4. Model Selection\n",
        "\n",
        "### Feature Selection\n",
        "\n",
        "Feature selection is a powerful technique for debiasing. We'll look at the effect of dropping each feature in turn.\n",
        "\n",
        "We are reaching the end , Stay Strong !!\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/anilkumarpanda/fairness_cb/development/img/red_cat.JPG\" alt=\"Alt Text\" height=\"400\">\n",
        "\n",
        "### Excercise\n",
        "1. This code takes a while a run, in the meanwhile can you come up with an\n",
        "easier way to identify features that can be adding addtional bias to the model ?\n",
        "\n",
        "Hint : Adversarial Models ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16f2a138-b820-45ca-83e8-2ac0988308da",
      "metadata": {
        "id": "16f2a138-b820-45ca-83e8-2ac0988308da",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# We'll examine the effect of dropping each feature individually on model performance and adverse impact ratios.\n",
        "features_to_drop = ['original model'] + features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afc683a9-938b-409a-8c5e-e9910962630d",
      "metadata": {
        "id": "afc683a9-938b-409a-8c5e-e9910962630d",
        "tags": []
      },
      "outputs": [],
      "source": [
        "num_cv_folds = 5\n",
        "\n",
        "# Build the custom cross-validation iterable.\n",
        "all_indices = np.arange(0, len(train))\n",
        "all_indices = np.random.permutation(all_indices)\n",
        "splits = np.array([int(np.floor(len(train)/num_cv_folds)) for _ in range(num_cv_folds-1)])\n",
        "splits = np.append(splits, len(train) - splits.sum())\n",
        "\n",
        "test_indices = np.split(all_indices, splits.cumsum())[:-1]\n",
        "test_groups = [train.iloc[test_ind]['RACE'].values for test_ind in test_indices]\n",
        "\n",
        "train_indices = [np.array([i for i in all_indices if i not in test_ind]) for test_ind in test_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ifQ766Gk6EB",
      "metadata": {
        "id": "4ifQ766Gk6EB"
      },
      "outputs": [],
      "source": [
        "feature_selection_results = pd.DataFrame(index=features_to_drop,\n",
        "                                         columns=['AUC', 'Black AIR', 'Hispanic AIR'])\n",
        "\n",
        "for dropped_feature in features_to_drop:\n",
        "\n",
        "    new_features = list(set(features).difference(set([dropped_feature])))\n",
        "\n",
        "    cv_auc = []\n",
        "    cv_black_air = []\n",
        "    cv_hispanic_air = []\n",
        "\n",
        "    for fold_num, (train_ind, test_ind) in enumerate(zip(train_indices, test_indices)):\n",
        "\n",
        "        new_monotone_constraints={k: v for k, v in dict(zip(features, monotone_constraints)).items() if k != dropped_feature}\n",
        "\n",
        "        cv_model = xgb.XGBClassifier(n_estimators=150,\n",
        "                                     max_depth=5,\n",
        "                                     learning_rate=0.05,\n",
        "                                     subsample=0.6,\n",
        "                                     colsample_bytree=1.0,\n",
        "                                     monotone_constraints=new_monotone_constraints,\n",
        "                                     random_state=12345,\n",
        "                                     use_label_encoder=False,\n",
        "                                     base_score=params['base_score'],\n",
        "                                     eval_metric='logloss',n_jobs=8)\n",
        "\n",
        "        train_slice = train.reset_index(drop=True).iloc[train_ind].copy()\n",
        "        test_slice = train.reset_index(drop=True).iloc[test_ind].copy()\n",
        "\n",
        "        cv_model = cv_model.fit(train_slice[new_features],\n",
        "                                train_slice[target])\n",
        "        y_pred = cv_model.predict_proba(test_slice[new_features])[:, 1]\n",
        "\n",
        "\n",
        "        model_metrics = perf_metrics(y_true=train[target].values[test_ind], y_score=y_pred)\n",
        "        best_cut = model_metrics.loc[model_metrics['f1'].idxmax(), 'cutoff']\n",
        "\n",
        "        cv_auc.append(sklearn.metrics.roc_auc_score(y_true=train[target].values[test_ind],\n",
        "                                                    y_score=y_pred))\n",
        "\n",
        "        test_slice['pred'] = y_pred\n",
        "        disparity_table = fair_lending_disparity(test_slice, y=target, yhat=f'pred',\n",
        "                           demo_name='RACE', groups=race_levels, reference_group='white',\n",
        "                           cutoff=best_cut)\n",
        "\n",
        "        cv_black_air.append(disparity_table.loc['black']['AIR'])\n",
        "        cv_hispanic_air.append(disparity_table.loc['hispanic']['AIR'])\n",
        "\n",
        "    feature_selection_results.loc[dropped_feature] = [np.mean(cv_auc),\n",
        "                                                      np.mean(cv_black_air),\n",
        "                                                      np.mean(cv_hispanic_air)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D10vBcPJqo61",
      "metadata": {
        "id": "D10vBcPJqo61",
        "tags": []
      },
      "outputs": [],
      "source": [
        "feature_to_drop = feature_selection_results.sort_values('Black AIR').index[-1]\n",
        "feature_to_drop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc19036f-e6ff-49d9-97bd-eee58ff876c0",
      "metadata": {
        "id": "cc19036f-e6ff-49d9-97bd-eee58ff876c0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "feature_selection_results.sort_values('Black AIR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2881d68e-e5da-4ea8-b9a2-2efa2f6b351c",
      "metadata": {
        "id": "2881d68e-e5da-4ea8-b9a2-2efa2f6b351c",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Drop the most 'problematic feature' calculate model scores.\n",
        "new_features = list(set(features).difference(set([feature_to_drop])))\n",
        "new_monotone_constraints={k: v for k, v in dict(zip(features, monotone_constraints)).items() if k in new_features}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e22ff40-f598-4cbd-9222-3b445f5b9a0f",
      "metadata": {
        "id": "8e22ff40-f598-4cbd-9222-3b445f5b9a0f"
      },
      "source": [
        "### Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ec1b684-a223-4980-acad-cbb6f2d35ac3",
      "metadata": {
        "id": "7ec1b684-a223-4980-acad-cbb6f2d35ac3",
        "tags": []
      },
      "outputs": [],
      "source": [
        "parameter_distributions = {\n",
        "    'n_estimators': np.arange(10, 221, 30),\n",
        "    'max_depth': [3, 4, 5, 6, 7],\n",
        "    'learning_rate': stats.uniform(0.01, 0.1),\n",
        "    'subsample': stats.uniform(0.7, 0.3),\n",
        "    'colsample_bytree': stats.uniform(0.5, 0.5),\n",
        "    'reg_lambda': stats.uniform(0.1, 50),\n",
        "    'monotone_constraints': [new_monotone_constraints],\n",
        "    'base_score': [params['base_score']]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fd339e8-39d6-448c-9d3e-4f7751737359",
      "metadata": {
        "id": "4fd339e8-39d6-448c-9d3e-4f7751737359",
        "tags": []
      },
      "outputs": [],
      "source": [
        "fold_number = -1\n",
        "\n",
        "def black_air(y_true, y_pred):\n",
        "\n",
        "\n",
        "    global fold_number\n",
        "    fold_number = (fold_number + 1) % num_cv_folds\n",
        "\n",
        "    model_metrics = perf_metrics(y_true, y_score=y_pred)\n",
        "    best_cut = model_metrics.loc[model_metrics['f1'].idxmax(), 'cutoff']\n",
        "\n",
        "    data = pd.DataFrame({'RACE': test_groups[fold_number],\n",
        "                        'y_true': y_true,\n",
        "                        'y_pred': y_pred},\n",
        "                        index=np.arange(len(y_pred)))\n",
        "\n",
        "    disparity_table = fair_lending_disparity(data, y='y_true', yhat='y_pred',\n",
        "                       demo_name='RACE', groups=race_levels, reference_group='white',\n",
        "                       cutoff=best_cut)\n",
        "\n",
        "    return disparity_table.loc['black']['AIR']\n",
        "\n",
        "\n",
        "def hispanic_air(y_true, y_pred):\n",
        "\n",
        "\n",
        "    global fold_number\n",
        "\n",
        "    model_metrics = perf_metrics(y_true, y_score=y_pred)\n",
        "    best_cut = model_metrics.loc[model_metrics['f1'].idxmax(), 'cutoff']\n",
        "\n",
        "    data = pd.DataFrame({'RACE': test_groups[fold_number],\n",
        "                        'y_true': y_true,\n",
        "                        'y_pred': y_pred},\n",
        "                        index=np.arange(len(y_pred)))\n",
        "\n",
        "    disparity_table = fair_lending_disparity(data, y='y_true', yhat='y_pred',\n",
        "                       demo_name='RACE', groups=race_levels, reference_group='white',\n",
        "                       cutoff=best_cut)\n",
        "\n",
        "    return disparity_table.loc['hispanic']['AIR']\n",
        "\n",
        "scoring = {\n",
        "        'AUC': 'roc_auc',\n",
        "        'Black AIR': sklearn.metrics.make_scorer(black_air, needs_proba=True),\n",
        "        'Hispanic AIR': sklearn.metrics.make_scorer(hispanic_air, needs_proba=True)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Excersise : Increase the no.of iterations.\n",
        "# Increase the no.of iterations to 50.\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "26FFPsfdmBhe"
      },
      "id": "26FFPsfdmBhe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54512802-7fdc-4039-a168-78efd84c4170",
      "metadata": {
        "id": "54512802-7fdc-4039-a168-78efd84c4170",
        "tags": []
      },
      "outputs": [],
      "source": [
        "grid_search = sklearn.model_selection.RandomizedSearchCV(xgb.XGBClassifier(random_state=12345,\n",
        "                                                                           use_label_encoder=False,\n",
        "                                                                           eval_metric='logloss'),\n",
        "                                                         parameter_distributions,\n",
        "                                                         n_iter=10,\n",
        "                                                         scoring=scoring,\n",
        "                                                         cv=zip(train_indices, test_indices),\n",
        "                                                         refit=False,\n",
        "                                                         error_score='raise').fit(train[new_features], train[target].values)\n",
        "results = pd.DataFrame(grid_search.cv_results_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "007d3e14-d6ce-4b6d-afb8-05c5a9c9641b",
      "metadata": {
        "id": "007d3e14-d6ce-4b6d-afb8-05c5a9c9641b",
        "tags": []
      },
      "outputs": [],
      "source": [
        "original_auc = feature_selection_results.loc['original model']['AUC']\n",
        "original_black_air = feature_selection_results.loc['original model']['Black AIR']\n",
        "\n",
        "new_auc = feature_selection_results.loc[feature_to_drop]['AUC']\n",
        "new_black_air = feature_selection_results.loc[feature_to_drop]['Black AIR']\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(original_black_air, 1.0, s=50, color='red', label='original model')\n",
        "ax.scatter(results['mean_test_Black AIR'], results['mean_test_AUC']/original_auc, label='post-hyperparameter tuning', marker='x')\n",
        "ax.scatter(new_black_air, new_auc/original_auc, color='orange', label='post-feature selection', marker='v')\n",
        "ax.legend(loc='lower left')\n",
        "ax.set_xlabel('Black AIR')\n",
        "ax.set_ylabel(f'AUC (normalized)')\n",
        "\n",
        "#fig.savefig('../Data/Data/Figures/model_tuning_scatter.svg', dpi=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08976d5d-9fbe-44d9-89c2-4d77569de9d7",
      "metadata": {
        "id": "08976d5d-9fbe-44d9-89c2-4d77569de9d7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "results[['mean_test_Black AIR', 'mean_test_Hispanic AIR', 'mean_test_AUC']].sort_values(['mean_test_Black AIR', 'mean_test_AUC'], ascending=False)\n",
        "highest_air_idx = results.loc[results['mean_test_Black AIR'] == results['mean_test_Black AIR'].max()].index[0]\n",
        "\n",
        "# We can also choose the fairest model that demonstrates no more than a 1% decrease in AUC from the original model.\n",
        "business_viable_models = results.loc[results['mean_test_AUC'] >= 0.99*original_auc]\n",
        "alternative_model_idx = business_viable_models.loc[business_viable_models['mean_test_Black AIR'] == business_viable_models['mean_test_Black AIR'].max()].index[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "069ea30d-cf0d-4041-bfb0-b2040be1a6b6",
      "metadata": {
        "id": "069ea30d-cf0d-4041-bfb0-b2040be1a6b6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "new_hyperparameter_idx = [highest_air_idx, alternative_model_idx]\n",
        "\n",
        "tuned_params = ['_'.join(col.split('_')[1:]) for col in results.columns if col.startswith('param_')]\n",
        "new_hyperparameters = [dict(zip(tuned_params, [row[f'param_{param}'] for param in tuned_params])) for _, row in results.loc[new_hyperparameter_idx].iterrows()]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7ff7fb3-d12d-47fc-9a90-58a94b907877",
      "metadata": {
        "id": "e7ff7fb3-d12d-47fc-9a90-58a94b907877"
      },
      "source": [
        "### Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fcf5043-68cc-4b72-bcde-873ae6925656",
      "metadata": {
        "id": "7fcf5043-68cc-4b72-bcde-873ae6925656",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def model_summary(y_true, y_pred, group_info, reference_group, metric_dict, cutoff,\n",
        "                  confusion_metrics_to_show=['False Positive Rate']):\n",
        "    \"\"\"\n",
        "    Function to put all things together.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    groups = np.unique(group_info)\n",
        "    protected_groups = [group for group in groups if group != reference_group]\n",
        "\n",
        "    model_metrics = perf_metrics(y_true, y_score=y_pred)\n",
        "    # best_cut = model_metrics.loc[model_metrics['f1'].idxmax(), 'cutoff']\n",
        "    # f1 = model_metrics['f1'].max()\n",
        "\n",
        "    f1 = model_metrics.loc[model_metrics['cutoff'] == cutoff, 'f1'].values[0]\n",
        "\n",
        "    auc = sklearn.metrics.roc_auc_score(y_true, y_score=y_pred)\n",
        "\n",
        "    data = pd.DataFrame({'demo': group_info,\n",
        "                        'y_true': y_true,\n",
        "                        'y_pred': y_pred},\n",
        "                        index=np.arange(len(y_pred)))\n",
        "\n",
        "    disparity_table = fair_lending_disparity(data, y='y_true', yhat='y_pred',\n",
        "                       demo_name='demo', groups=groups, reference_group=reference_group,\n",
        "                       cutoff=cutoff)\n",
        "\n",
        "    airs = dict(zip([f'{group} AIR' for group in protected_groups],\n",
        "                    [disparity_table.loc[group]['AIR'] for group in protected_groups]))\n",
        "\n",
        "    confusion_mats = {level: get_confusion_matrix(data, 'y_true', 'y_pred', by='demo',\n",
        "                                                  level=level, cutoff=cutoff) for level in groups}\n",
        "    confusion_metrics = confusion_matrix_metrics(confusion_mats, metric_dict)\n",
        "    confusion_disparity_frame = confusion_metrics/confusion_metrics.loc[reference_group, :]\n",
        "\n",
        "    confusion_metric_disparities = dict()\n",
        "    for metric in confusion_metrics_to_show:\n",
        "        confusion_metric_disparities.update(dict(zip([f\"{group} {metric} Disparity\" for group in protected_groups],\n",
        "                                               [confusion_disparity_frame.loc[group][metric] for group in protected_groups])))\n",
        "\n",
        "    output = {'AUC': auc, 'F1': f1}\n",
        "    output.update(airs)\n",
        "    output.update(confusion_metric_disparities)\n",
        "\n",
        "    return pd.Series(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e486922-b7fb-4c8c-9f63-6eca9c0485e6",
      "metadata": {
        "id": "1e486922-b7fb-4c8c-9f63-6eca9c0485e6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "feature_selection_model = xgb.XGBClassifier(n_estimators=150,\n",
        "                                     max_depth=5,\n",
        "                                     learning_rate=0.05,\n",
        "                                     subsample=0.6,\n",
        "                                     colsample_bytree=1.0,\n",
        "                                     monotone_constraints=new_monotone_constraints,\n",
        "                                     random_state=12345,\n",
        "                                     use_label_encoder=False,\n",
        "                                     base_score=params['base_score'],\n",
        "                                     eval_metric='logloss').fit(train[new_features], train[target])\n",
        "\n",
        "model_metrics = perf_metrics(y_true=valid[target],\n",
        "                             y_score=feature_selection_model.predict_proba(valid[new_features])[:, 1])\n",
        "best_cut_feature_selection = model_metrics.loc[model_metrics['f1'].idxmax(), 'cutoff']\n",
        "\n",
        "hyp_tuning_model_1 = xgb.XGBClassifier(random_state=12345,\n",
        "                                     use_label_encoder=False,\n",
        "                                     eval_metric='logloss', **new_hyperparameters[0]).fit(train[new_features], train[target])\n",
        "\n",
        "model_metrics = perf_metrics(y_true=valid[target],\n",
        "                             y_score=hyp_tuning_model_1.predict_proba(valid[new_features])[:, 1])\n",
        "best_cut_hyp_1 = model_metrics.loc[model_metrics['f1'].idxmax(), 'cutoff']\n",
        "\n",
        "hyp_tuning_model_2 = xgb.XGBClassifier(random_state=12345,\n",
        "                                     use_label_encoder=False,\n",
        "                                     eval_metric='logloss', **new_hyperparameters[1]).fit(train[new_features], train[target])\n",
        "\n",
        "\n",
        "model_metrics = perf_metrics(y_true=valid[target],\n",
        "                             y_score=hyp_tuning_model_2.predict_proba(valid[new_features])[:, 1])\n",
        "best_cut_hyp_2 = model_metrics.loc[model_metrics['f1'].idxmax(), 'cutoff']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9844eac-d5df-44a8-9c24-676023479c57",
      "metadata": {
        "id": "d9844eac-d5df-44a8-9c24-676023479c57",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model_predictions = {'Original Model': (test[f\"p_{target}\"].values, best_cut_original),\n",
        "                    'Pre-processing (reweighting)': (test[f'p_{target}_reweighted'].values, reweighted_best_cut),\n",
        "                    'Feature selection': (feature_selection_model.predict_proba(test[new_features])[:, 1], best_cut_feature_selection),\n",
        "                    'HPT :1 Fairest Model': (hyp_tuning_model_1.predict_proba(test[new_features])[:, 1], best_cut_hyp_1),\n",
        "                    'HPT :2 Balance Fairness & Performance': (hyp_tuning_model_2.predict_proba(test[new_features])[:, 1], best_cut_hyp_2)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8832a731-cf3d-486a-ac39-e6087191a9cd",
      "metadata": {
        "id": "8832a731-cf3d-486a-ac39-e6087191a9cd",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model_summaries = {name: model_summary(test[target].values, pred, test['RACE'].values, 'white', metric_dict, cutoff) for name, (pred, cutoff) in model_predictions.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Time to make a choice :\n",
        "Put your money on the models!\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/anilkumarpanda/fairness_cb/development/img/banker_cat.JPG\" alt=\"Alt Text\" height=\"400\">"
      ],
      "metadata": {
        "id": "Bu6MiWebsvgW"
      },
      "id": "Bu6MiWebsvgW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9789fc9-51a6-4fe3-b26e-0022522442e4",
      "metadata": {
        "id": "c9789fc9-51a6-4fe3-b26e-0022522442e4",
        "tags": [],
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(model_summaries)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "224cb364",
      "metadata": {
        "id": "224cb364"
      },
      "source": [
        "\n",
        "\n",
        "### References :\n",
        "\n",
        "Most of the code and knowledge is borrowed from the following sources :\n",
        "\n",
        "1. [Fairness Tree](https://docs.google.com/presentation/d/1ycNhDJr5uQiBhWvdlNArnv7ojydW73qryOjRBB30Z44/edit#slide=id.g8d5290cc44_0_417)\n",
        "2. [Machine Learning for High-Risk Applications](https://learning.oreilly.com/library/view/machine-learning-for/9781098102425/)\n",
        "3. [SHAP for Fairness](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/Explaining%20quantitative%20measures%20of%20fairness.html)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Qp290ypsp-u"
      },
      "id": "1Qp290ypsp-u",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}